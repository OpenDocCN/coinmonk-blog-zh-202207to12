<html>
<head>
<title>Understanding Topological Data Analysis(Graph Theory)</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">了解拓扑数据分析(图论)</h1>
<blockquote>原文：<a href="https://medium.com/coinmonks/understanding-topological-data-analysis-graph-theory-245c332fa8c6?source=collection_archive---------20-----------------------#2022-07-25">https://medium.com/coinmonks/understanding-topological-data-analysis-graph-theory-245c332fa8c6?source=collection_archive---------20-----------------------#2022-07-25</a></blockquote><div><div class="ef hh hi hj hk hl"/><div class="hm hn ho hp hq"><div class=""/><figure class="fi fk ir is it iu fe ff paragraph-image"><div role="button" tabindex="0" class="iv iw di ix bf iy"><div class="fe ff iq"><img src="../Images/1b64e0af67393097fd0a8b7e2c8391a1.png" data-original-src="https://miro.medium.com/v2/resize:fit:1400/format:webp/1*cpF-JfBL4Wp-d2D4PAC8Aw.jpeg"/></div></div><figcaption class="jb jc fg fe ff jd je bd b be z ek">Photo by <a class="ae jf" href="https://unsplash.com/@redviking509?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">John Anvik</a> on <a class="ae jf" href="https://unsplash.com/s/photos/thread?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText" rel="noopener ugc nofollow" target="_blank">Unsplash</a></figcaption></figure><ol class=""><li id="cb42" class="jg jh ht ji b jj jk jl jm jn jo jp jq jr js jt ju jv jw jx dt translated"><strong class="ji hu"> DB-LSH:具有基于查询的动态桶的位置敏感散列(</strong><a class="ae jf" href="https://arxiv.org/pdf/2207.07823" rel="noopener ugc nofollow" target="_blank"><strong class="ji hu">arXiv</strong></a><strong class="ji hu">)</strong></li></ol><p id="00b6" class="pw-post-body-paragraph jy jz ht ji b jj jk ka kb jl jm kc kd jn ke kf kg jp kh ki kj jr kk kl km jt hm dt translated"><strong class="ji hu">作者:</strong><a class="ae jf" href="https://arxiv.org/search/?searchtype=author&amp;query=Tian%2C+Y" rel="noopener ugc nofollow" target="_blank"/><a class="ae jf" href="https://arxiv.org/search/?searchtype=author&amp;query=Zhao%2C+X" rel="noopener ugc nofollow" target="_blank"/><a class="ae jf" href="https://arxiv.org/search/?searchtype=author&amp;query=Zhou%2C+X" rel="noopener ugc nofollow" target="_blank">周晓芳</a></p><p id="f095" class="pw-post-body-paragraph jy jz ht ji b jj jk ka kb jl jm kc kd jn ke kf kg jp kh ki kj jr kk kl km jt hm dt translated"><strong class="ji hu">摘要:</strong>在高维近似最近邻(ANN)搜索问题的众多解决方案中，位置敏感哈希(LSH)以其亚线性的查询时间和对查询精度的鲁棒理论保证而闻名。传统的LSH方法可以从哈希表中快速生成少量的候选项，但会遇到大的索引大小和哈希边界问题。最近解决这些问题的研究通常会产生额外的开销来识别合格的候选对象或消除误报，从而使查询时间不再是次线性的。为了解决这一难题，本文提出了一种新的LSH方案，称为DB-LSH，它支持对大型高维数据集进行有效的人工神经网络搜索。它使用多维索引组织投影空间，而不是使用固定宽度的哈希桶。我们的方法可以通过避免为不同的桶大小维护许多散列表来显著降低空间成本。在DB-LSH的查询阶段，通过基于索引的窗口查询动态构建所需宽度的基于查询的超立方桶，可以高效地生成少量高质量的候选。对于近似比为c的n维数据集，我们的严格理论分析表明，DB-LSH实现了更小的查询代价O(nρ∫dlogn)，其中ρ∫的界为1/cα，而现有工作中的界为1/c。在真实世界数据上的大量实验证明了DB-LSH在效率和准确性上优于最先进的方法</p><p id="c5ad" class="pw-post-body-paragraph jy jz ht ji b jj jk ka kb jl jm kc kd jn ke kf kg jp kh ki kj jr kk kl km jt hm dt translated"><strong class="ji hu"> 2。BCD:使用位置敏感散列算法(</strong><a class="ae jf" href="https://arxiv.org/pdf/2112.05492" rel="noopener ugc nofollow" target="_blank"><strong class="ji hu">arXiv</strong></a><strong class="ji hu">)</strong>的跨架构二进制比较数据库实验</p><p id="aa57" class="pw-post-body-paragraph jy jz ht ji b jj jk ka kb jl jm kc kd jn ke kf kg jp kh ki kj jr kk kl km jt hm dt translated"><strong class="ji hu">作者:</strong> <a class="ae jf" href="https://arxiv.org/search/?searchtype=author&amp;query=Tan%2C+H" rel="noopener ugc nofollow" target="_blank">郝希坦</a></p><p id="89dc" class="pw-post-body-paragraph jy jz ht ji b jj jk ka kb jl jm kc kd jn ke kf kg jp kh ki kj jr kk kl km jt hm dt translated"><strong class="ji hu">摘要:</strong>给定一个没有源代码的二进制可执行文件，很难通过逆向工程来确定二进制文件中的每个函数是做什么的，如果没有之前的经验和上下文就更难了。在本文中，我们比较了不同哈希函数在检测LLVM IR代码的相似提升片段方面的有效性，并介绍了跨架构二进制代码相似性搜索数据库框架的设计和实现，该框架使用MinHash作为选择的哈希算法，基于SimHash、SSDEEP和TLSH。其动机是帮助逆向工程师通过与已知函数的数据库进行比较，快速获得未知二进制文件中函数的上下文。这个项目的代码是开源的，可以在https://github.com/h4sh5/bcddb<a class="ae jf" href="https://github.com/h4sh5/bcddb" rel="noopener ugc nofollow" target="_blank">找到</a></p><p id="bdc0" class="pw-post-body-paragraph jy jz ht ji b jj jk ka kb jl jm kc kd jn ke kf kg jp kh ki kj jr kk kl km jt hm dt translated"><strong class="ji hu"> 3。面向在线稀疏大数据分析的局部敏感哈希聚合非线性邻域矩阵分解(</strong><a class="ae jf" href="https://arxiv.org/pdf/2111.11682" rel="noopener ugc nofollow" target="_blank"><strong class="ji hu">arXiv</strong></a><strong class="ji hu">)</strong></p><p id="86bc" class="pw-post-body-paragraph jy jz ht ji b jj jk ka kb jl jm kc kd jn ke kf kg jp kh ki kj jr kk kl km jt hm dt translated"><strong class="ji hu">作者:</strong> <a class="ae jf" href="https://arxiv.org/search/?searchtype=author&amp;query=Li%2C+Z" rel="noopener ugc nofollow" target="_blank">李</a>，<a class="ae jf" href="https://arxiv.org/search/?searchtype=author&amp;query=Li%2C+H" rel="noopener ugc nofollow" target="_blank">郝莉</a>，<a class="ae jf" href="https://arxiv.org/search/?searchtype=author&amp;query=Li%2C+K" rel="noopener ugc nofollow" target="_blank">李垦利</a>，<a class="ae jf" href="https://arxiv.org/search/?searchtype=author&amp;query=Wu%2C+F" rel="noopener ugc nofollow" target="_blank">吴梵</a>，<a class="ae jf" href="https://arxiv.org/search/?searchtype=author&amp;query=Chen%2C+L" rel="noopener ugc nofollow" target="_blank">陈迪雅</a>，<a class="ae jf" href="https://arxiv.org/search/?searchtype=author&amp;query=Li%2C+K" rel="noopener ugc nofollow" target="_blank">李克勤</a></p><p id="0904" class="pw-post-body-paragraph jy jz ht ji b jj jk ka kb jl jm kc kd jn ke kf kg jp kh ki kj jr kk kl km jt hm dt translated"><strong class="ji hu">摘要:</strong>矩阵分解(MF)可以从高维数据中提取低秩特征，整合数据流形分布的信息，可以考虑非线性的邻域信息。因此，MF在稀疏大数据的低秩分析方面引起了广泛关注，例如协同过滤(CF)推荐系统、社交网络和服务质量。然而，存在以下两个问题:1)用于构建图相似矩阵(GSM)的巨大计算开销，以及2)用于中间GSM的巨大存储开销。因此，基于GSM的MF，例如，核MF、图正则化MF等。，不能直接应用于云和边缘平台上稀疏大数据的低秩分析。为了解决稀疏大数据分析中的这个棘手问题，我们提出了位置敏感哈希(LSH)聚合MF (LSH-MF)，它可以解决以下问题:1)LSH-MF提出的概率投影策略可以避免GSM的构建。此外，LSH-MF可以满足稀疏大数据精确投影的要求。2)为了在GPU上运行LSH-MF进行细粒度并行化和在线学习，我们还提出了适用于CUDA并行化的CULSH-MF。实验结果表明，CULSH-MF不仅减少了计算时间和内存开销，而且获得了更高的准确率。与深度学习模型相比，CULSH-MF不仅可以节省训练时间，而且可以达到相同的准确率。</p><p id="937d" class="pw-post-body-paragraph jy jz ht ji b jj jk ka kb jl jm kc kd jn ke kf kg jp kh ki kj jr kk kl km jt hm dt translated"><strong class="ji hu"> 4。经由位置敏感散列的次线性最小二乘值迭代(</strong><a class="ae jf" href="https://arxiv.org/pdf/2105.08285" rel="noopener ugc nofollow" target="_blank"><strong class="ji hu">arXiv</strong></a><strong class="ji hu">)</strong></p><p id="eeee" class="pw-post-body-paragraph jy jz ht ji b jj jk ka kb jl jm kc kd jn ke kf kg jp kh ki kj jr kk kl km jt hm dt translated"><strong class="ji hu">作者:</strong> <a class="ae jf" href="https://arxiv.org/search/?searchtype=author&amp;query=Shrivastava%2C+A" rel="noopener ugc nofollow" target="_blank">安舒马利</a>、<a class="ae jf" href="https://arxiv.org/search/?searchtype=author&amp;query=Song%2C+Z" rel="noopener ugc nofollow" target="_blank">、</a>、<a class="ae jf" href="https://arxiv.org/search/?searchtype=author&amp;query=Xu%2C+Z" rel="noopener ugc nofollow" target="_blank">徐</a></p><p id="b03c" class="pw-post-body-paragraph jy jz ht ji b jj jk ka kb jl jm kc kd jn ke kf kg jp kh ki kj jr kk kl km jt hm dt translated"><strong class="ji hu">摘要:</strong>我们提出了第一个可证明的最小二乘值迭代(LSVI)算法，其运行时间复杂度与动作数量呈次线性关系。我们将值迭代中的值函数估计过程公式化为一个近似的最大内积搜索问题，并提出了一个位置敏感散列(LSH)[因迪克和莫特瓦尼·STOC ' 98，安多尼和拉森斯泰恩·STOC ' 15，安多尼，拉阿霍文，拉森斯泰恩和温加藤·索达' 17]类型的数据结构来解决这个具有次线性时间复杂度的问题。此外，我们在近似最大内积搜索理论和强化学习的后悔分析之间建立了联系。我们证明，通过选择近似因子，我们的次线性LSVI算法保持了与原始LSVI算法相同的遗憾，同时将运行时间复杂度降低到次线性。据我们所知，这是第一个将LSH和强化学习相结合的工作，带来了可证明的改进。我们希望我们结合数据结构和迭代算法的新方法将为进一步研究优化中的成本降低打开大门。</p><p id="0304" class="pw-post-body-paragraph jy jz ht ji b jj jk ka kb jl jm kc kd jn ke kf kg jp kh ki kj jr kk kl km jt hm dt translated"><strong class="ji hu"> 5。对于流形学习，深度神经网络可以是位置敏感的哈希函数(</strong><a class="ae jf" href="https://arxiv.org/pdf/2103.06875" rel="noopener ugc nofollow" target="_blank"><strong class="ji hu">【arXiv】</strong></a><strong class="ji hu">)</strong></p><p id="f245" class="pw-post-body-paragraph jy jz ht ji b jj jk ka kb jl jm kc kd jn ke kf kg jp kh ki kj jr kk kl km jt hm dt translated"><strong class="ji hu">作者:</strong></p><p id="286a" class="pw-post-body-paragraph jy jz ht ji b jj jk ka kb jl jm kc kd jn ke kf kg jp kh ki kj jr kk kl km jt hm dt translated"><strong class="ji hu">摘要:</strong>众所周知，训练深度神经网络给出了捕捉输入的基本特征的有用表示。然而，这些表述在理论和实践中却鲜为人知。在监督学习的背景下，一个重要的问题是这些表示是否捕捉到用于分类的信息特征，同时过滤掉非信息的噪声特征。我们通过考虑一个生成过程来探索这个问题的形式化，其中每个类都与一个高维流形相关联，并且不同的类定义不同的流形。在该模型下，每个输入使用两个潜在向量产生:(I)一个“流形标识符”γ和；(ii)~沿流形表面移动示例的“变换参数”θ。例如，γ可以代表狗的标准图像，而θ可以代表姿势、背景或光照的变化。我们提供的理论和经验证据表明，神经表征可以被视为类LSH函数，它将每个输入映射到一个嵌入，该嵌入仅是信息γ的函数，并且对θ不变，从而有效地恢复流形标识符γ。这种行为的一个重要后果是一次性学会看不见的类。</p><blockquote class="kn"><p id="1126" class="ko kp ht bd kq kr ks kt ku kv kw jt ek translated">交易新手？试试<a class="ae jf" rel="noopener" href="/coinmonks/crypto-trading-bot-c2ffce8acb2a">密码交易机器人</a>或者<a class="ae jf" rel="noopener" href="/coinmonks/top-10-crypto-copy-trading-platforms-for-beginners-d0c37c7d698c">复制交易</a></p></blockquote></div></div>    
</body>
</html>